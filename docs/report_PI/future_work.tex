\section{Future Work}
\label{future_work}
\indent


The biggest problem of the linear algebra implementation can be the intermediate matrices, if a column has no nonzero element, they are not optimal for further parallelisations. This is the unique speedup 

limitation

regarding the relational algebra approach. If that limitation is overcome the speedups would climb to more significant values when compared to the relational algebra version. \par 
It is also clear that the keys for selection significantly interfere with both linear and relational algebra algorithms. In the LA specific case, a selection operation that returns a low number of nonzero elements might compromise the attainable speedup through parallelisation, since there might not be enough data to keep the computational units busy. \par 

There is still room to greatly improve the parallelisation of this algorithm. For example,  a gather/scatter analysis for distributed memory parallelism might result in larger attainable speedup. However, efforts need to me made to detect possible latency or bandwidth issues. \par 
Another approach is to duplicate data in a shared memory environment . For the CSC format, regarding the three arrays, every thread would have the total CSC column pointer array and a portion of CSC values and CSC row indexes arrays. The computation of both splitted arrays would be thread independent and there would only be necessary to reduce the CSC column pointer. \par 

To improve data locality the Block Compressed Sparse Row (BSR) Format could be used. However, it would largely increase the algorithm complexity for the defined methods. That improvement should only be used in the CSR linear algebra defined version.\par 

Future work also includes extending the scope of both offline and run-time optimisations. These include:
\begin{itemize} 
\item to fully translate the remaining TPC-H queries and benchmark both linear and relational algebra approaches;
\item to investigate reordering methods to reduce total query compute time;
\item to exploit index compression, further cache-blocking and TLB blocking to reduce memory traffic and to further improve locality;
\item to implement other matrix partitioning schemes;
\item to improve load balance when there are different data structures for each generated matrix;
\item to reduce data structure conversion costs at run-time, for the CSR linear algebra version;
\item to determine the most efficient number of cores for parallelism.
\end{itemize}

The usage of CUDA and MIC systems should also be explored as an heterogenous system solution. Since the algorithms require huge portions of simple computation, and it is already parallelised via OpenMP, porting the application to be Xeon Phi compatible should not present great challenges.\par 


 

